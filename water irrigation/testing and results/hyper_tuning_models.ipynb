{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ganes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preproccess_scale_transform(csv_filepath):\n",
    "    COLUMN_ORDER = [\n",
    "        \"SM_4\",\n",
    "        \"Time\",\n",
    "        \"ST_4\",\n",
    "        \"temp\",\n",
    "        \"humidity\",\n",
    "        \"precip\",\n",
    "        \"windspeed\",\n",
    "        \"cloudcover\",\n",
    "        \"conditions\",\n",
    "    ]\n",
    "\n",
    "    dataset = pd.read_csv(csv_filepath)\n",
    "\n",
    "    dataset.drop(\n",
    "        [\n",
    "            \"Date\",\n",
    "            \"SM_2\",\n",
    "            \"SM_8\",\n",
    "            \"SM_20\",\n",
    "            \"SM_40\",\n",
    "            \"ST_2\",\n",
    "            \"ST_8\",\n",
    "            \"ST_20\",\n",
    "            \"ST_40\",\n",
    "            \"solarenergy\",\n",
    "            \"precipprob\",\n",
    "            \"preciptype\",\n",
    "            \"snow\",\n",
    "            \"snowdepth\",\n",
    "            \"windgust\",\n",
    "            \"winddir\",\n",
    "            \"sealevelpressure\",\n",
    "            \"visibility\",\n",
    "            \"solarenergy\",\n",
    "            \"uvindex\",\n",
    "            \"severerisk\",\n",
    "            \"icon\",\n",
    "            \"stations\",\n",
    "            \"dew\",\n",
    "            \"solarradiation\",\n",
    "            \"Time\",\n",
    "            # \"cloudcover\",\n",
    "            \"feelslike\",\n",
    "            # \"windspeed\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    dataset.dropna(inplace=True)\n",
    "    # dataset = dataset[COLUMN_ORDER]\n",
    "    # dataset[\"Time\"] = dataset[\"Time\"].apply(lambda x: int(x[:2]))\n",
    "\n",
    "    X = dataset.iloc[:, 1:].values\n",
    "    # print(X[0])\n",
    "\n",
    "    y = dataset.iloc[:, 0].values\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[(\"encoder\", OneHotEncoder(), [-1])], remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    X = np.array(ct.fit_transform(X))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    X_train[:, 9:] = sc.fit_transform(X_train[:, 9:])\n",
    "\n",
    "    X_test[:, 9:] = sc.transform(X_test[:, 9:])\n",
    "\n",
    "    X_train, y_train, X_test, y_test = map(\n",
    "        np.asarray, [X_train, y_train, X_test, y_test]\n",
    "    )\n",
    "\n",
    "    X_train, X_test = map(lambda x: x.astype(\"float32\"), [X_train, X_test])\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_preproccess_scale_transform(\n",
    "    \"C:/Users/ganes/OneDrive/Documents/GitHub/Project_LISA/water irrigation/datasets/prototype_final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "ANN_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(8, activation=\"relu\"),\n",
    "        keras.layers.Dense(4, activation=\"relu\"),\n",
    "        keras.layers.Dense(2, activation=\"relu\"),\n",
    "        keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ANN_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.mean_squared_error,\n",
    "    metrics=keras.metrics.mean_absolute_error,\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "history = ANN_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "ANN_model.summary()\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ypred = ANN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Increase maximum depth\n",
    "dtr_model = DecisionTreeRegressor(\n",
    "    max_depth=10, random_state=0, min_samples_split=20)\n",
    "\n",
    "# Fine-tune hyperparameters\n",
    "dtr_model.fit(X_train, y_train)\n",
    "\n",
    "dtr_ypred = dtr_model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "dtr_mae = mean_absolute_error(y_test, dtr_ypred)\n",
    "dtr_mse = mean_squared_error(y_test, dtr_ypred)\n",
    "print(\"DTR MAE:\", dtr_mae)\n",
    "print(\"DTR MSE:\", dtr_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Try different kernels\n",
    "svr_models = [\n",
    "    SVR(kernel=\"rbf\", C=10, gamma=0.1),\n",
    "    SVR(kernel=\"linear\", C=10),\n",
    "    SVR(kernel=\"poly\", C=10, degree=3),\n",
    "]\n",
    "\n",
    "# Fine-tune hyperparameters\n",
    "for model in svr_models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    svr_ypred = model.predict(X_test)\n",
    "\n",
    "    svr_mae = mean_absolute_error(y_test, svr_ypred)\n",
    "    svr_mse = mean_squared_error(y_test, svr_ypred)\n",
    "\n",
    "    print(f\"SVR {model.kernel} MAE: {svr_mae}, MSE: {svr_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Feature selection using L1 regularization\n",
    "mlr_model = LinearRegression()\n",
    "mlr_model.fit(X_train, y_train)\n",
    "\n",
    "mlr_ypred = mlr_model.predict(X_test)\n",
    "\n",
    "mlr_mae = mean_absolute_error(y_test, mlr_ypred)\n",
    "mlr_mse = mean_squared_error(y_test, mlr_ypred)\n",
    "print(\"MLR MAE:\", mlr_mae)\n",
    "print(\"MLR MSE:\", mlr_mse)\n",
    "\n",
    "# Consider adding polynomial features\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "mlr_model_poly = LinearRegression()\n",
    "mlr_model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "mlr_poly_ypred = mlr_model_poly.predict(X_test_poly)\n",
    "\n",
    "mlr_poly_mae = mean_absolute_error(y_test, mlr_poly_ypred)\n",
    "mlr_poly_mse = mean_squared_error(y_test, mlr_poly_ypred)\n",
    "print(\"MLR with Poly Features MAE:\", mlr_poly_mae)\n",
    "print(\"MLR with Poly Features MSE:\", mlr_poly_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.scatter(y_test[:150], y_test[:150], color=\"red\")\n",
    "plt.scatter(y_test[:150], ann_ypred[:150], color=\"green\")\n",
    "plt.title(\"ANN_model\")\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.scatter(y_test[:150], y_test[:150], color=\"red\")\n",
    "plt.scatter(y_test[:150], dtr_ypred[:150], color=\"green\")\n",
    "plt.title(\"dtr_model\")\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.scatter(y_test[:150], y_test[:150], color=\"red\")\n",
    "plt.scatter(y_test[:150], svr_ypred[:150], color=\"green\")\n",
    "plt.title(\"svr_models\")\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.scatter(y_test[:150], y_test[:150], color=\"red\")\n",
    "plt.scatter(y_test[:150], mlr_ypred[:150], color=\"green\")\n",
    "plt.title(\"mlr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Mean Squared Error on Test Set: 809.7291405159345\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Create a GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to perform the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb_regressor,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set using the best hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
